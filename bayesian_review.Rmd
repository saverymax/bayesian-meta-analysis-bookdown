# Bayesian methods for Meta-analysis

## Why Bayesian?

Fixed effect models for meta-analysis are suitable when the studies are homogeneous and can be assumed to share an overall effect. However, as described in @Higgins-2009, fixed effect models fail when the overall effect is heterogenous between studies. It is then necessary to turn to random effect models so as to be able to estimate study-specific effects. However, to fully take advantage of these models, such as for the purpose of generating posteriors and uncertainty intervals around the study-specific effects or to incorporate prior information, Bayesian methods are required. Bayesian methods allow us to combine a likelihood based on observed data and a prior distribution model which incorporates previous beliefs and historical information. In the prior, we can incorporate information from previous studies, our beliefs about the size of the effect we are studying, or use the prior to make a conservative estimate [@Rhodes-2016]. In this way, using a Bayesian approach we can expand the random effects model to more carefully consider the observed effects from any given meta-analysis. 

Furthermore, the use of Bayesian methods lends itself to cases in which the sample size is relatively small. When we underestimate the between-study variance, we produce over-confident estimates, in that our confidence intervals are too small [@Harrer-2021]. Furthermore, Bayesian estimation methods avoid estimates between-study variance that approach 0, which, in the Maximum Likelihood or Restriced Maximum Likelihood context, is not uncommon when estimating random-effects models with small sample sizes [@Chung-2013]. Given the advantages of the random effects model in the Bayesian framework, as opposed to the fixed effects model, we now proceed to describe Bayesian inference in the context of meta-analysis and random effects models. 

\subsection{Bayesian introduction}

Here we provide a brief introduction to Bayesian methods in general and then describe their use in conducting meta analyses.

In the Bayesian framework, the likelihood of observed data is combined with a prior distribution that represents historical information. This is commonly written as
\begin{gather}
p(\theta|y) \propto p(y|\theta)p(\theta)
(\#eq:bayes)
\end{gather}
where $\theta$ represents a vector of the parameters in our model and $y$ the vector of observations [@Gelman-2013]. This distribution is referred to as the posterior density. $\propto$ represents the proportionality due to the normalizing constant which makes the posterior a proper density. 

To generate samples from this posterior distribution, various sampling techniques can be used. Due to the background required to explain these methods and the focus of this report on meta-analysis, the sampling techniques are not discussed in detail here. However, because we will conduct a meta-analysis using the Bayesian modelling software Stan [@Carpenter-2017], it is worth mentioning the sampler used there: the No-U-Turn Sampler (NUTS) is used [@Homan-2014], an improvement on the Hamiltonian Monte Carlo method.

Following [@Williams-2018], we now describe the hierarchical model used for conducting a random effects Bayesian meta-analysis. In this formulation, each study will have a particular mean and variance

\begin{gather}
y_i \sim N(\theta_i, \sigma_i^2) \\
\theta_i \sim N(\mu, \tau^2) \\
(\mu, \tau^2) \sim prior(.)
\end{gather}

where $y_i$ is the effect we observe in each study $i=1,...,N$, $\theta_i$ is the mean effect within the context of study $i$, $\sigma_i^2$ is the variance within that study, $\mu$ is the mean effect of the studies, and $\tau^2$ is the between-study variance. The effect for a given study $i$ is drawn from $N(\mu, \tau^2)$, as opposed to being fixed (as in the fixed effect model). Notice that the effect from each study, $\theta_i$, shares a distribution $N(\mu, \tau^2)$ with all other studies [@Harrer-2021] 

Using Equation \@ref(eq:bayes), the joint posterior density of the random effects model will be
\begin{gather}
p(\theta, \mu, \tau^2|y) \propto p(y|\theta, \mu, \tau^2)p(\theta, \mu, \tau^2) = \prod_{i=1}^{k}p(y_i|\theta_i,\sigma_i^2)p(\theta_i|\mu,\tau^2)p(\mu, \tau^2)
\end{gather}

We then must consider which priors to use for $\mu$ and $\tau^2$. This is a fairly important question. @Gelman-2017 discuss a case in which the choice of a uniform prior leads to completely incorrect posterior estimates and therefore misleading conclusions of a study. They state 
\begin{quote}
The dominance of the information encoded in the measurement depends not only on the size of
the data but also on the structure of the likelihood and the effect being studied. The more complex the
likelihood and the smaller the effect being considered, the more data are needed to render the prior irrelevant [@Gelman-2017].
\end{quote}

This implies that in the small-sample world of meta-analysis, the prior is going to have a disproportionate effect. 

The priors for the parameters in our model can be chosen with various justifications. We discuss two "camps" of priors: those that are informative and those that are ostensibly less informative--which we will refer to as weakly informative. We will briefly discuss informative priors and then spend the rest of this review discussing and applying weakly informative priors. 

## Informative priors

An informative prior can be considered one which is based on results from previous studies. For example, both @Turner-2012 and @Turner-2015 generate predictive distributions from previous Cochrane reviews of meta-analyses. These predictive distributions can be used as informative priors for conducting new meta-analyses. In @Turner-2015, the informative priors are created for a variety of different types of binary-outcome studies, such as disease outcome or infection onset. The priors are informative in that they use estimates generated from modelling the meta-analysis data as the parameters in the priors. 

@Rhodes-2015 also generate predictive distributions from previous meta-analyses using Cochrane reviews. In contrast to @Turner-2015, they focus on priors for continuous outcomes. They conduct a prior sensitivity analysis, demonstrating how the use of informative priors can dramatically change the between-study variance estimate, either increasing or reducing $\tau^2$ depending on the context.

Taking a a different approach, @Rhodes-2016 consider how to generate informative priors using data-augmentation techniques. Specifically, they generate new data from inverse-gamma distributions based on previous studies, and add this generated data to the observed data. This approach actually stands in contrast to the fully Bayesian approach we have discussed so far, in that they are not actually using a prior for parameters in their model but are instead augmenting the observed data with previous data. This is a philosophically similar but mathematically different technique.

## Weakly informative priors

As mentioned, the choice of prior is a fairly important decision, but also a somewhat contentious one. The reason for this is that the choice brings together arguments from statistical theory, philosophy, and domain-specific expertise. There are certain statistical properties that must be considered in the choice of the prior, namely conjugacy and impropriety of the posterior. But these mathematical properties don't dilute the real-world phenomenon that we are modelling, and thus philosophical arguments regarding what type of information the prior provides and expert opinion regarding the relevance of the information provided by the prior must always be taken into account.

Weakly informative priors are those that do not consider information from previous studies but still constrain the estimates of the parameters to a realistic or meaningful range of values. They often have desirable mathematical properties as well. That is to say, all priors come with their own set of assumptions, so that there is no such thing as a non-informative prior, but it is possible to select priors that contribute less information than others @Gelman-2017. 

We will discuss three weakly informative priors previously used for Baysesian meta-analysis: inverse-gamma, half-t, and half-cauchy distributions. Following this discussion, we will apply these priors in our own prior sensitivity analysis.

The inverse-gamma prior for $\tau^2$ can be written as

\begin{gather}
    p(\tau^2) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}(\tau^2)^{-(\alpha+1)}\exp\Bigg(\frac{\beta}{\tau^2}\Bigg)
\end{gather}

It is often used as a weakly informative prior due to the fact that it is conjugate for $\tau^2$ in the random effects model. However, the posterior is sensitive to the value of $\alpha$ and $\beta$ when $\tau^2$ is small [@Gelman-2006]. This means that the inverse-gamma is not strictly weakly informative, as it incorporates information based on ones choice of $\alpha$ and $\beta$. A weakly informative prior should be invariant to this sort of hyperparameter selection. Relatedly, @Polson-2012 argue that the inverse-gamma distribution pulls $\tau^2$ values away from 0, which nullifies its weakly informative properties in the case where $\tau^2$ is small. 

We next consider the half-t prior. The half-t distribution is a special case of the folded-t distribution where the $\mu$ parameter (not shown) is equal to 0 \footnote{See \url{https://en.wikipedia.org/wiki/Folded-t_and_half-t_distributions}}. @Gelman-2006 make the case for the use of this prior as a replacement to the inverse-gamma, in order to avoid the issues with inverse-gamma when $\tau^2$ is small. The half-t is defined as 

\begin{gather}
    p(\tau) \propto \Bigg(1 + \frac{1}{\nu}\Big(\frac{\tau}{2}\Big)^2\Bigg)^{-(\nu+1)/2}
(\#eq:half-t)
\end{gather}

The half-cauchy distribution is a special case of the half-t distribution (and therefore the folded-t), where $\nu=1$ in Equation \@ref(eq:half-t). More exactly, it is defined as
\begin{gather}
    p(\tau) = \frac{(\tau)^{-1/2}(1+\tau^2)^{-1}}{\beta(1/2,1/2)}
\end{gather}

where $\beta$ refers to the Beta function. It can allow for wider tails than the half-t, meaning that in the case of random effect models, it allows for larger estimates of the between-study variance [@Sheng-2017].

We have presented three weakly informative prior distributions that can realistically constrain the between-study variance, while also maintaining some semblance of mathematical justification in the context of the likelihoods used in Bayesian meta-analysis; this does not, however, shake the sense of the somewhat arbitrary nature of these choices. To this--abeit regarding only the half-cauchy--@Polson-2012 say, 
\begin{quote}
    A natural question is: of all the hypergeometric inverted-beta priors, why choose the half-Cauchy? There is no iron-clad reason to do so, of course, and we can imagine many situations where subjective information would support a different choice. But in examining many other members of the class, we have observed that the half-Cauchy seems to occupy a sensible middle ground in terms of frequentist risk. 
\end{quote}

Generalizing from this, each prior occupies the space of compromise between any staunch set of assumptions regarding the appropriate parameter values. Beyond that, one must apply such priors to their own meta-analysis of interest, and determine if, yes, indeed, these priors lead to reasonable results. That is, then, the next step that we take.
